[
  {
    "objectID": "nbs/2020/09/2020-09-05-lesson-1.html",
    "href": "nbs/2020/09/2020-09-05-lesson-1.html",
    "title": "fastai v4: Lesson 1",
    "section": "",
    "text": "Notes from fastai lesson 1.\n\n\n\nLesson 1 Video\nfastai Forum\n\n\n\n\n\nDiscussed what’s needed to take this course (not much!).\nDiscussed history of AI and how deep neural networks came to be. Mostly news to me, 1/10.\nIntroduced Jupyter, ipywidgets, REPL. Mostly seen this before, 7/10.\nIntroduced ML (repeat, but still new, 4/10)\n\ntake an input, process it, get an output.\nSamuel’s terminology: take inputs & weights into a model, generate results\nAdd feedback: measure performance of the results (a metric), then change weights. Rinse & repeat.\nDifferent weights to the model allows it to do a different task\nUniversal approximation theorem: theory that a neural network could solve any problem to any level of accuracy.\nNeed a way to update weights - SGD: stochastic gradient descent - to update the weights\n\nML Limitations\n\nA model can only be created from data\nA model can only learn from patterns in the inputs\nA model can only make a prediction - actions happen externally\n\nLabeled data is key and often missing - good part, bad part, etc.\nIt is important to note that a feedback loop can be created, resulting in a causal relationship where none existed before. Jeremy gave an example in lesson 1. This goes to ethics along ML and understanding inherent biases in your training dataset that you may not be aware of.\nThe fastai notebook\nIntro to fastai. Discuss REPL, data sets, etc.\nfrom fastai.vision.all import *\nlearner = cnn_learner(dls, resnet34, metrics=error_rate)\nHere cnn_learner is a function that generates our model, dls is our dataset, resnet34 is our architecture, and error_rate is a loss function for feedback.\nresnet34 is a predefined neural network trained on images that is free to use. 34 indicates it has 34 layers, more layers requires more GPU memory.\nerror_rate is computed on data not used in training, also known as a holdout set or validation set, to help avoid overfitting. Might need to increase the size of the holdout set to avoid overfitting. The ambiguiuty is actually important in model building, otherwise the model will only know how to recognize images it has seen before.\nOther uses\nsegmentation: figuring out what every pixel in an image is (what label it corresponds to)\nFor a PWA, labels might be\n\ntrace, screw, via, component\n\nA larger training set might allow\n\ntrace, via, capacitor, resistor, inductor, transistor, integrated circuit, etc.\n\nProject idea: create an architecture for use in image segmentation that is trained on images of various PWAs (raspberry Pis, motherboards, etc.) to recognize features.\ntabular data: fitting a model to predict salary based on a variety of parameters, predicting ratings a user might give a movie they haven’t seen based on previous ratings they have given (known as collaboration, used in recommendation engines)\n\n\n\n\n\n\n\n\n\n\n\nKeyword\nDescription\n\n\n\n\nArchitecture\nThe “program” we are running. Often synonymous with model, but represents its functional form..\n\n\nParameters\nThe “weights” into the “program” that alter its performance\n\n\nPredictions\nThe output of our architecture, computed from independent variables which does not include labels\n\n\nLabels\nThe targets or dependent variables, assumed to be true for a given prediction.\n\n\nLoss\nA metric of performance we measure our model by: how well did our prediction (computed from independent variables) match our labels (our dependent variables)?\n\n\nModel\nThe combination of the parameters and architecture that can act on inputs to generate a prediction.\n\n\nInputs\nThe data on which the model acts to generate a prediction. No inputs (data) == No predictions!\n\n\nAction\nThe decision made from reasoning about a given prediction.\n\n\nTransfer learning\nUsing a pre-trained model for a task other than what it was originally intended for (via training)\n\n\nFine Tuning\nA transfer learning technique that updates the parameters of a pretrained model by training for additional epochs using a different task from that used for pretraining. This is also known as fitting.\n\n\nhead\nThe last and newly added layer of a model which trains it on a particular data set. This replaces the previous layer when cnn_learner is used.\n\n\nepoch\nOne complete training of the model on the complete dataset\n\n\nOver fitting\nWhen a model learns the unique characteristics of a dataset rather than a generalization, limiting its ability to do inference on new data.\n\n\n\nHoward, Jeremy. Deep Learning for Coders with fastai and PyTorch . O’Reilly Media. Kindle Edition.\nFrom the loss, we can /update/ the weight for a given input, thus allowing our system to learn.\n\n\n\nDo the questionaire, run the notebooks, etc.\n\n\nSome of these questions were answered after watching both videos and reading chapter 1 of the book. Although this blog post is titled lesson 1, chapter 1 is covered in the first two lessons. Since I’m consuming the material one lesson at a time I’ll continue to follow this pattern.\n\n(If you’re not sure of the answer to a question, try watching the next lesson and then coming back to this one, or read the first chapter of the book.)\n\n\nDo you need these for deep learning?\nLots of math T / F\n\nFalse: Much of the math is taken for you thanks to libraries such as numpy.\n\nLots of data T / F\n\nFalse: Relatively small data sets can train world class models - no need for “Big Data”! Lots of expensive computers T / F\nFalse: You can use consumer grade hardware if you have it or take advantage of Google Cloud, AWS, Colab, etc.\n\nA PhD T / F\n\nFalse: You will need some knowledge of programming and understanding of your data set, but a PhD is not needed.\n\nName five areas where deep learning is now the best in the world.\n\nNLP (natural language processing)\nComputer Vision\nMedicine\nBiology\nImage generalization\nForecasting\nRobotics (grip strength, movement)\n\nWhat was the name of the first device that was based on the principle of the artificial neuron?\n\nThe Mark I Perceptron (sounds like something from Fallout!)\n\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\n\nA set of processing units\nA state of activation\nAn output function for each unit\nA pattern of connectivity between units\nA propagation rule for patterns to pass between units via a pattern of connectivity\nAn activation rule that controls the output for a given input and current state of a unit\nA learning rule that modifies patterns of connectivity\nAn environment to operate in (often overlooked)\n\nWhat were the two theoretical misunderstandings that held back the field of neural networks?\n\nThat because a single layer of neurons could not simulate an XOR, neural networks were a dead end\nDNN would be too big and too slow to be useful\n\nWhat is a GPU?\n\nA Graphcis Processing Unit, designed for massively parallel execution of floating point vectors.\n\nOpen a notebook and execute a cell containing: 1+1. What happens?\n\nIt computes and displays the output (2)\n\nFollow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen.\n\nDone in the notebook…\n\nComplete the Jupyter Notebook online appendix.\n\nDone\n\nWhy is it hard to use a traditional computer program to recognize images in a photo?\n\nRequires a pixel by pixel search of the image, or something akin to a kernel to review pixel groupings, with significant comparison and cyclic complexity. Here, we tell the computer what to think, when to think it - we are explicit. This is converse to machine learning, which learns the patterns in the data and uses those patterns to recognize images in a photo.\n\nWhat did Samuel mean by “weight assignment”?\n\nWeight assignments are variables used in a model to alter the performance of the program (model). The weight assignments must have an automatic means to be updated, so as to provide feedback to improve the performance of the model for its respective purpose.\n\nWhat term do we normally use in deep learning for what Samuel called “weights”?\n\nWe call them “parameters”.\n\nDraw a picture that summarizes Samuel’s view of a machine learning model.\n\n* results = model(inputs, weights)\n* performance = results - actuals\n* weights = performance(weights)\n\nEssentially, the inputs of a model are used to generate results. Those results have a certain performance, which is improved by modifying the weights and repeating the operation.\n\nWhy is it hard to understand why a deep learning model makes a particular prediction?\n\nBecause of the complexity of the “black box” - it would be necessary to instrument each layer of a neural network in order to understand why. In other words, the why is the entirety of the state machine, although some parts of the state machine may be better than others.\n\nWhat is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?\n\nUniversal approximation theorem (reminds me of using taylor series expansion to approximate various mathematical functions)\n\nWhat do you need in order to train a model?\n\nYou need a dataset and an architecture (a pretrained model)\n\nHow could a feedback loop impact the rollout of a predictive policing model?\n\nInherent bias in the dataset could lead to increased policing of a particular population representative of the bias, resulting in additional policing, enhancing the existing dataset bias.\n\nDo we always have to use 224×224-pixel images with the cat recognition model?\n\nNo, but larger images or sizes may impact performance of the GPU (meaning it will take longer for the model to learn)\n\nWhat is the difference between classification and regression?\n\nClassification predicts the category of a given input (think predicting finite enumerations)\nRegression predicts a future numeric value, like the temperature tomorrow\n\nWhat is a validation set? What is a test set? Why do we need them?\n\nA validation or hold out set is a subset of a dataset that we do not train the model with, but we use to grade the model’s performance.\nA test set is a dataset we do not show even ourselves, to avoid introducing bias through EDA and model training.\nWe need these to avoid overfitting or introducing bias into our models\n\nWhat will fastai do if you don’t provide a validation set?\n\nIt will automatically use a subset of your data as the validation set.\n\nCan we always use a random sample for a validation set? Why or why not?\n\nIf we used a random sample for validation, then repeated training (e.g. resampling the random data set and training the model on the resampled data) we could overfit our model.\nIf we use a static sample for validation, then repeated training won’t be able to overfit, because it will never learn from the validation set.\nFor example, with time series data we may hold out a specific period (say the last two weeks) of data as a validation set, rather than a random sampling of the dataset..\n\nWhat is overfitting? Provide an example.\n\nOverfitting is the process of a model “memorizing” a given dataset. This can happen when our learning rate continues to increase and our error rate tends to zero. For example, if you overtrained a model on your entire dataset of cats, it would not be able to perform inference on new cat photos it has never seen before.\n\nWhat is a metric? How does it differ from “loss”?\n\nA metric is all we care about: it is how well our model performs on our validation set.\n\nHow can pretrained models help?\n\nPretrained models can be used to train a new model (transfer learning)\n\nWhat is the “head” of a model?\n\nThe head of the model is the layer who’s parameters are modified by training\n\nWhat kinds of features do the early layers of a CNN find? How about the later layers?\n\nThis depends, but a CNN might first learn things like basic shapes or color gradients. Later layers might recognize repeating patterns or more specific shapes.\n\nAre image models only useful for photos?\n\nNo! It’s possible to create images from many different types of data, including log files, executables, sound, etc. This can be done by transforming data to grayscale, or using existing visualization techniques (such as FFT).\n\nWhat is an “architecture”?\n\nAn architecture is a pretrained model from a specific dataset. resnet is an example of an architecture used in this book for computer vision.\n\nWhat is segmentation?\n\nsegmentation is the label of individual pixels of data in an image, e.g. applying the label “car” to the parts of an image with a car in it.\n\nWhat is y_range used for? When do we need it?\n\ny_range\n\nWhat are “hyperparameters”?\n\nA hyperparameter is a parameter about a parameter. When choosing new hyper parameters we must be careful or we may inadvertently introduce bias or overfitting the validation data, basically “finding the answer we want to hear”.\n\nWhat’s the best way to avoid failures when using AI in an organization?\n\nTo understand why a validation and test data set are needed and to ensure they are properly used when developing a model.\n\n\n\n\n\n\nI skipped these because I thought the answers were easy, but decided to return to them for completeness.\n\nWhy is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning?\n\nGPU’s contain thousands of massively parallel execution cores optimized for floating point arithmetic and matrix math. They contain relatively small amounts of extremely high bandwidth memory. These properties make them advantageous for DL, but inhibit them from being used for, say, running a Web Browser. CPU’s are general purpose computing devices, containing substantially fewer cores, slower memory speed, and tend to favor integer math over floating point math or matrix math (although Intel has tried to change this with AVX512…poorly). A key performance indicator for both CPU and GPUs that tend to lead to effective DL is the FLOP (floating point operations). GPUs typically have substantially higher FLOPs than CPUs.\n\nTry to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice.\n\nElections: if a model with a feedback loop is used to predict election outcomes and has substantial influence on a population of voters, it could dissentivize voters from turning out for an election. While this might be most visible for American presedential races, it could affect less publicized elections such as those for ballot measures and Congress. I don’t think Five-Thirty-Eight is this…yet.\nWith the recent Facebook news and Russian influence in American Democracy, I think we’re beginning to see a trend of adversarial machine learning, which is exploiting a competitors production models to destablize or influence its outcome. I think nation states and other actors will continue to use feedback loops in active learning systems.\nSearch results. I Google’d a whiskey bottle to read a review before purchasing it. Now Google shows me whiskey reviews in my news feed, displacing other news articles. Reading these seems to reinforce the model and increases the number of articles I see. Since the number of articles I can read is limited, there is no way to “tone down” the model without telling it I’m simply not interested in whiskey. It’s kind of interesting because in general you implicitly train Google. You can’t implicitly untrain it (e.g. there is no decay function or “forgetting”, or it operates on a time horizon I havent seen yet)."
  },
  {
    "objectID": "nbs/2020/09/2020-09-06-fossil-dataset.html",
    "href": "nbs/2020/09/2020-09-06-fossil-dataset.html",
    "title": "Fossil Dataset Construction",
    "section": "",
    "text": "I hacked together most of this code in between completing the first few lessons from the fastai course v3. The final product is a site I host internally so that my local paleontologist can label data for me. :)\nThis was the first big step in one of my projects. The objective of this notebook was to learn about the reddit API and build a dataset I could use to train a model using the resnet architecture. It also helped me validate my home setup was complete.\nWhat it didn’t teach me is how to integrate with the broader community, which is another reason I wanted to create this blog.\n\nimport os\nimport sys\nimport re\nimport glob\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport requests\n\n# For generating the widgets\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\nfrom IPython.display import clear_output, display, Image as Img\nfrom IPython.core.display import HTML\nimport ipyplot\n\nfrom fastai.vision import *\nfrom fastai.vision import core\n\n# For interacting with reddit\nimport praw\nfrom praw.models import MoreComments\n\n# For managing our reddit secrets\nfrom dotenv import load_dotenv\nload_dotenv()\n\n\n\nTo get started, we will follow the example in the first lessons by using a dataset that is labeled by the directory name. We will store images in the path below, which we will also use for training with fastai.\nIn this case, images are saved to the path /home/tbeck/src/data/fossils. fastai saves the images using an 8-digit, zero filled identifier. This code does not check for duplicates and does not allow the user to review the existing dataset (e.g. to clean it), although there are some tools now with fastai that might be useful for that purpose.\n\ndataset_path = Path('/home/tbeck/src/data/fossils')\nlabels = [x.name for x in dataset_path.iterdir() if (x.is_dir() and x.name not in ['models'])]\n\n\n\n\nWe must instantiate the reddit PRAW client, so we do so by passing it environment variables loaded from a .env file via the python-dotenv package. Using the API, we can obtain URLs to posted images and scrape the reddit comments (useful for getting hints).\n\nreddit = praw.Reddit(client_id=os.environ['REDDIT_CLIENT_ID'], client_secret=os.environ['REDDIT_SECRET'],\n                     password=os.environ['REDDIT_PASSWORD'], user_agent=os.environ['REDDIT_USER_AGENT'],\n                     username=os.environ['REDDIT_USERNAME'])\n\nfossilid = reddit.subreddit('fossilid')\n\nWe need some helper functions to retrieve the images and save them to our dataset. I found that URLs obtained from reddit need some post processing, otherwise they do not render properly.\n\ndef download_image(url, dest=None):\n    \"\"\"Given a URL, saves the image in a format the fastai likes.\"\"\"\n    dest = Path(dest)\n    dest.mkdir(exist_ok=True)\n    \n    files = glob.glob(os.path.join(dest, '*.jpg')) + glob.glob(os.path.join(dest, '*.png'))\n    i = len(files)\n\n    suffix = re.findall(r'\\.\\w+?(?=(?:\\?|$))', url)\n    suffix = suffix[0] if len(suffix)>0  else '.jpg'\n    \n    try: core.download_url(url, dest/f\"{i:08d}{suffix}\", overwrite=True, show_progress=False)\n    except Exception as e: f\"Couldn't download {url}.\"   \n\ndef get_image(url, verbose=False):\n    \"\"\"Given a URL, returns the URL if it looks like it's a URL to an image\"\"\"\n    IMG_TEST = \"\\.(jpg|png)\"\n    p = re.compile(IMG_TEST, re.IGNORECASE)\n    if p.search(url):\n        if verbose:\n            print(\"url to image\")\n        return url\n    \n    IMGUR_LINK_TEST = r\"((http|https)://imgur.com/[a-z0-9]+)$\"\n    p = re.compile(IMGUR_LINK_TEST, re.IGNORECASE)\n    \n    if p.search(url):\n        if verbose:\n            print(\"imgur without extension\")\n        return url + '.jpg'\n\n    IMGUR_REGEX_TEST = r\"((http|https)://i.imgur.com/[a-z0-9\\.]+?(jpg|png))\"\n    p = re.compile(IMGUR_REGEX_TEST, re.IGNORECASE)\n    \n    if p.search(url):\n        if verbose:\n            print(\"imgur with extension\")\n        return url\n    \n    return None\n\nclass Error(Exception):\n    def __init__(self, msg):\n        self.msg = msg\n        \nclass SubmissionStickiedError(Error):\n    pass\nclass SubmissionIsVideoError(Error):\n    pass\nclass SubmissionNotAnImageError(Error):\n    pass\nclass DisplayError(Error):\n    pass\n\nNow we can query reddit for the images. The method below to build the dataset is a little clunky (I create arrays for each column of data and take great steps to be sure they are equal length). A better way would be to delegate creating this data structure to a single function so that the code below is less complex.\n\n# Fetch submissions for analysis and initialize parallel arrays\nsubmissions = []\nimages = []\ntop_comments = []\nerrors = []\nverbose = False\n\nfor i, submission in enumerate(reddit.subreddit(\"fossilid\").new(limit=None)):\n    submissions.append(submission)\n    images.append(None)\n    top_comments.append(None)\n    errors.append(None)\n    \n    try:\n        if submission.stickied:\n            raise SubmissionStickiedError(\"Post is stickied\")\n            \n        if submission.is_video:\n            raise SubmissionIsVideoError(\"Post is a video\")\n\n        if get_image(submission.url):\n            if verbose:\n                print(f\"Title: {submission.title}\")\n            \n            images[i] = get_image(submission.url)\n\n            try:\n                if verbose:\n                    display(Img(get_image(submission.url), retina=False, height=400, width=400))\n            except Exception as err:\n                if verbose:\n                    print(f\"Failed to retrieve transformed image url {get_image(submission.url)} from submission url {submission.url}\")\n                raise DisplayError(f\"Failed to retrieve transformed image url {get_image(submission.url)} from submission url {submission.url}\")\n\n            submission.comments.replace_more(limit=None)\n            for top_level_comment in submission.comments:\n                if verbose:\n                    print(f\"Comment: \\t{top_level_comment.body}\")\n                top_comments[i] = top_level_comment.body\n                break\n        else:\n            raise SubmissionNotAnImageError(\"Post is not a recognized image url\")\n    except Exception as err:\n        submissions[i] = None\n        images[i] = None\n        top_comments[i] = None\n        errors[i] = err.msg\n\ndf = pd.DataFrame({'submissions': submissions, 'images': images, 'comments': top_comments, 'errors': errors})\ndf.dropna(how='all', inplace=True)\ndf.dropna(subset=['images'], inplace=True)"
  },
  {
    "objectID": "nbs/2020/09/2020-09-06-fossil-dataset.html#reviewing-the-dataset",
    "href": "nbs/2020/09/2020-09-06-fossil-dataset.html#reviewing-the-dataset",
    "title": "Fossil Dataset Construction",
    "section": "Reviewing The Dataset",
    "text": "Reviewing The Dataset\nBecause the data is stored in a DataFrame, we can easily manipulate the information we’ve scraped. The ipyplot package is useful for generating thumbnails from a series of urls. This makes it easy to quickly review what’s been scraped.\n\nipyplot.plot_images(df['images'], max_images=80, img_width=100)"
  },
  {
    "objectID": "nbs/2020/09/2020-09-18-fossils-production.html",
    "href": "nbs/2020/09/2020-09-18-fossils-production.html",
    "title": "Production from Scratch - fastai Lesson 3",
    "section": "",
    "text": "Retrieve files of our subject from bing image search. The images we search for are downloaded locally.\n\nkey = os.environ.get('AZURE_SEARCH_KEY', 'xyz')\n\n\nresults = search_images_bing(key, 'trilobite')\nims = results.attrgot('content_url')\nlen(ims)\n\n150\n\n\n\nfossil_types = 'trilobite', 'crinoid', 'bivalve'\npath = Path('critters')\n\n\nfor o in fossil_types:\n    dest = (path/o)\n    if not dest.exists():\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'{o} fossil')\n        download_images(dest, urls=results.attrgot('content_url'))\n\n\n\n\n\n\n\nSome post processing cleanup might be required to remove empty files, html data, or files that are encoded in a format that PIL cannot understand (such as VP8). They might say they are ‘.jpg’ files, but that does NOT mean that they are!\nfastai has a handy get_image_files() function that will recursively identify images and return them as a list and a verify_images() which returns a list of images that are not up to snuff. Then, we use L.map() to unlink (remove) any files in the failed images list.\n\nfns = get_image_files(path)\nfns\n\n(#437) [Path('critters/trilobite/00000001.jpg'),Path('critters/trilobite/00000000.jpg'),Path('critters/trilobite/00000004.jpg'),Path('critters/trilobite/00000005.jpg'),Path('critters/trilobite/00000002.jpg'),Path('critters/trilobite/00000007.jpg'),Path('critters/trilobite/00000006.jpg'),Path('critters/trilobite/00000013.jpg'),Path('critters/trilobite/00000008.jpg'),Path('critters/trilobite/00000011.jpg')...]\n\n\n\nfailed = verify_images(fns)\nfailed\n\n\n\n\n(#0) []\n\n\n\nfailed.map(Path.unlink);\n\nTime to take a look at the images we’ve downloaded. fastai has a handy show_batch() function for its ImageDataLoaders objects so we can get a preview of the images inside jupyter. We must specify item_tfms= or else the widget will not be able to render the wide variety of image resolutions that have been downloaded.\n\ndls = ImageDataLoaders.from_folder(path/'crinoid', valid_pct=0.2, item_tfms=Resize(256))\ndls.valid_ds.items[:3]\n\n[Path('critters/crinoid/00000087.jpg'),\n Path('critters/crinoid/00000065.jpg'),\n Path('critters/crinoid/00000045.jpg')]\n\n\n\ndls.show_batch(max_n=40, nrows=4)\n\n\n\n\n\ndls = ImageDataLoaders.from_folder(path/'trilobite', valid_pct=0.2, item_tfms=Resize(256))\ndls.show_batch(max_n=40, nrows=4)\n\n\n\n\n\ndls = ImageDataLoaders.from_folder(path/'bivalve', valid_pct=0.2, item_tfms=Resize(256))\ndls.show_batch(max_n=40, nrows=4)\n\n\n\n\nWe are now ready to create a DataBlock. The DataBlock will contain our images and labels; it will need to know how to ‘find’ the items, how to separate them into a training set and a validation set, where to get the dependent variable from (the label, e.g. the directory they are in), and last how to transform the image so that we can run CUDA on it.\n\ncritters = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=RandomResizedCrop(256, min_scale=0.3))\n\nThe data block has not yet seen our data, so lets show it the data.\n\ndls = critters.dataloaders(path, bs=64)\n\n\ndls.valid.show_batch(max_n=10, nrows=2)\n\n\n\n\n\ndls.train.show_batch(max_n=10, nrows=2)\n\n\n\n\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.940702\n      0.609582\n      0.244186\n      00:09\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.762174\n      0.398268\n      0.197674\n      00:12\n    \n    \n      1\n      0.567270\n      0.371190\n      0.151163\n      00:12\n    \n    \n      2\n      0.462896\n      0.349624\n      0.139535\n      00:12\n    \n  \n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(10, nrows=2)\n\n\n\n\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\nlearn.export()\n#path.ls(file_exts='.pkl')\n\n\np = Path()\np.ls(file_exts='.pkl')\n\n(#1) [Path('export.pkl')]\n\n\nI had some cleanup issues when using the ImageClassifierCleaner() class. After using the widget and then invoking the for loops to unlink and move the images, it threw an exception because filenames already existed. It appears it had problems counting the images/moving the files due to name collisions. It also wasn’t clear what to do after cleaning the dataset - simply running learn.fine_tune(1) did not work since the dataset had changed, so I reinitialized the datablock and retrained the model. This improved performance from an error rate of about 17% to about 14%."
  },
  {
    "objectID": "posts/2020/09/2020-09-05-the-story-so-far.html",
    "href": "posts/2020/09/2020-09-05-the-story-so-far.html",
    "title": "The story so far",
    "section": "",
    "text": "Ok, so this all started around July 2020 when I decided to invest in my own education and learn more about what some of my team members were working on. One of them suggested fast.ai so I decided to give it a try.\nAt the time, the course v3 was in “production”. Since then, v4 has been released.\n\n\nBeing the hands on engineer that I am, I decided to build my own system for machine learning from spare parts around the house. I already have experience deploying jupyter for use by development teams and I am no stranger to Linux. The fast.ai Getting Started guide originally suggested using managed services such as Google Cloud or AWS, but it was straightforward enough to get something working that I’ve documented it below.\nNote that this isn’t recommended by Jeremy in the video lesson, but I decided to do it to understand what is going on behind the scenes (and why pay AWS when I have the hardware and free solar energy).\n\n\nThese are parts I had laying around the house. The key if you want GPU acceleration with pytorch is to have the right GPU; too old and your GPU won’t support the necessary GPU compute capabilities. Below is the harware I had on hand:\n\nIntel i7-4770\n32 GB of RAM\nNVIDIA Corporation GM204 [GeForce GTX 970] (rev a1); GPU compute capability 5.2\n1 TB SSD (actually an upgrade; spinning rust was unbearably slow due to the low number of random IOPS)\n\n\n\n\nLinux is my preferred operating system in general for hacking, so I went with the recently released Ubuntu 20.04 LTS Server.\nI’ve been using anaconda as my python distribution of choice for many years now, so I grabbed the python 3.8 x86_64 for Linux package.\nI also grabbed docker.io. Nvidia has a solution for doing GPU compute that requires docker to be installed, as well.\nSince fastai uses pytorch, and pytorch only support CPU or GPU via cudatoolkit, I needed the nvidia.ko kernel module, the necessary nvidia cuda toolkit libraries, and the right packages in a conda environment.\nI installed nvidia-dkms-450 to provide nvidia drivers for my GTX 970.\nNvidia provides a CUDA toolkit 10.2 Ubuntu repo. Even though it says 18.04, it worked fine for me on 20.04. Just follow the instructions to install the cuda package.\nYou might also find the NVIDIA CUDA Installation Guide for Linux helpful for troubleshooting installation issues.\nFinally, to take advantage of the GPU you must install the GPU-accelerated version of pytorch. Only the conda instructions are needed since the system python isn’t used.\n$ conda install pytorch torchvision cudatoolkit=10.2 -c pytorch\nReboot as needed.\n\n\n\nFinally we can create a new conda environment, install the necessary packages, and verify pytorch can see and use the GPU.\n\nCreate a fastai conda environment with python 3.8\n$ conda create -n fastai python=3.8\n$ conda activate fastai\nInstall fastai in conda, as per their Install Instructions. It might take a while for conda to determine which channel to get the packages from.\n$ conda install -c fastai -c pytorch -c anaconda fastai gh anaconda\n$ conda install -c fastai -c pytorch -c anaconda fastai gh anaconda cudatoolkit=10.2\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /home/tbeck/anaconda3\n\n  added / updated specs:\n    - anaconda\n    - cudatoolkit=10.2\n    - fastai\n    - gh\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    ca-certificates-2020.7.22  |                0         132 KB  anaconda\n    certifi-2020.6.20          |           py37_0         159 KB  anaconda\n    conda-4.8.4                |           py37_0         3.0 MB  anaconda\n    cudatoolkit-10.2.89        |       hfd86e86_1       540.0 MB  anaconda\n    gh-0.11.1                  |                0         5.5 MB  fastai\n    openssl-1.1.1g             |       h7b6447c_0         3.8 MB  anaconda\n    ------------------------------------------------------------\n                                          Total:       552.5 MB\n\nThe following NEW packages will be INSTALLED:\n\n  gh                 fastai/linux-64::gh-0.11.1-0\n\nThe following packages will be SUPERSEDED by a higher-priority channel:\n\n  ca-certificates                                 pkgs/main --> anaconda\n  certifi                                         pkgs/main --> anaconda\n  conda                                           pkgs/main --> anaconda\n  cudatoolkit                                     pkgs/main --> anaconda\n  openssl                                         pkgs/main --> anaconda\n\n\nProceed ([y]/n)?\nOnce installed, you can quickly test that your GPU is seen and used from the command line like so:\n$ python -c 'import torch; print(torch.cuda.get_device_name())'\nGeForce GTX 970\n$ python -c 'import torch; print(torch.rand(2,3).cuda())'\ntensor([[0.3352, 0.0835, 0.5349],\n        [0.3712, 0.2851, 0.8767]], device='cuda:0')\n\nIf you see your expected video card and a tensor returned, you’re all set. If you have multiple GPU’s installed, you may need to specify which one to use. Check out this stack overflow article on how to set the appropriate environment variables for the command line and for jupyter to work.\n\n\n\nI’ve noticed that if you update conda using conda update --all, it will try to pull in the latest version of cudatoolkit, which as of this writing is cudatoolkit-11.0.221-h6bb024c_0. This is safe to do, but you will need to downgrade back to cudatoolkit-10.2. This seems to be due to how anaconda handles/prioritizes packages from various channels. Below is an example.\n\n\n$ conda update --all\n...\nThe following packages will be UPDATED:\n\n  cudatoolkit        anaconda::cudatoolkit-10.2.89-hfd86e8~ --> pkgs/main::cudatoolkit-11.0.221-h6bb024c_0\n\n\n\n$ conda install pytorch torchvision cudatoolkit=10.2 -c pytorch\n...\n  added / updated specs:\n    - cudatoolkit=10.2\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    cudatoolkit-10.2.89        |       hfd86e86_1       365.1 MB\n    ------------------------------------------------------------\n                                           Total:       365.1 MB\n\nThe following packages will be DOWNGRADED:\n\n  cudatoolkit                           11.0.221-h6bb024c_0 --> 10.2.89-hfd86e86_1\n\n\n\nIf you get the below trying to use torch then cuda isn’t working as expected.\n$ python -c 'import torch; print(torch.rand(2,3).cuda())'\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/home/tbeck/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 192, in _lazy_init\n    _check_driver()\n  File \"/home/tbeck/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 95, in _check_driver\n    raise AssertionError(\"Torch not compiled with CUDA enabled\")\nAssertionError: Torch not compiled with CUDA enabled\nNote that trying to install cudatoolkit=10.2 alone might result in this error, so be sure that pytorch and torchvision are included when specifying cudatoolkit=10.2.\n\n\n\n\nThere are two sets of notebooks for the class:\n\nThe fastbook, a guided set of notebooks with prose for following along in the videos: fastbook\nThe same notebooks as a study aid: course-v4\n\nPlease consider showing your support by buying the fastbook: Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD\nI setup my own area for hacking:\n$ mkdir ~/src\n$ cd ~/src && git clone https://github.com/fastai/fastbook.git && git clone https://github.com/fastai/course-v4.git\nNow that CUDA is working and we have the code, I prefer fire up my jupyter notebook in a screen session. To do this I generated a https://jupyter-notebook.readthedocs.io/en/stable/config.html via jupyte notebook --generate-config and wrote it to ~/.jupyter/jupyte_notebook_config.py. Then I made it listen on 0.0.0.0 so I can reach it from my LAN (or anywhere in the world via wireguard!).\nNow I just run screen, activate conda with conda activate fastai, and finally start jupyter with jupyter notebook. For tricks on using screen see this quickreference\nIf you prefer to use jupyter lab, you’ll need to conda install jupyterlab and run jupyter lab instead."
  },
  {
    "objectID": "posts/2020/09/2020-09-06-lesson-2.html",
    "href": "posts/2020/09/2020-09-06-lesson-2.html",
    "title": "fastai v4: Lesson 2",
    "section": "",
    "text": "Notes from fastai lesson 2.\n\n\n\nLesson 2 Video\nfastai Forum\nModel Zoo\nONNX Models\n\n\n\n\nTransfer learning: using an existing architecture to create a model trained on a particular data set\nError is one kind of metric, measuring “how well you’re doing” Loss is a measure of performance used to improve the parameters. “Are we learning (adjusting our parameters)?”\nModel training\n\nBuilt a model from Bing API search results (grizzly vs black vs teddy bears)\nUsed DataBlock() to create a dataset\nUsed dataloaders to load the data into memory\nUsed the resnet18 architecture to train a model using the dataset, similar to lesson 1\nExported the model via pickle to productionize it - model contains the architecture + new parameters + vocabulary (labels)\nUsed the model to perform inference on images the model has not seen before\nUsed ImageClassifierCleaner() to clean the dataset\n\nNext lesson\n\nDeploying to binder, treating your model as if it’s in production by uploading new images to it in “production”\n\n\n\n\n\n\n\n\n\n\n\nJargon We Use (again)\nDescription\n\n\n\n\nLabel\nThe data we’re trying to predict (recall the diagram)\n\n\nArchitecture\nA template of the model we are trying to fit. It represents the mathematical function we pass inputs & parameters to.\n\n\nModel\nAn architecture with a specific set of parameters. The parameters may be created through training over one or more epochs.\n\n\nParameters\nThe values of the model that we can alter trough training.\n\n\nFit or Train\nUpdating the parameters such that the model is better able to predict our labels.\n\n\nPretrained model\nA model with parameters adjusted via training, typically will be fine_tune()d, such as resnet34.\n\n\nfine tune\nUpdate a pretrained model for another task, such as making resnet34 recognize cats or dogs.\n\n\nepoch\nOne complete pass through the input data\n\n\nmetric\nA measure of how good the model is to control training via SGD\n\n\nvalidation set\nA subset of our data we do not train our model with to measure its performance\n\n\ntraining set\nA subset of our data we train our model with that does not include any data from the validation set\n\n\noverfitting\nTraining a model that results in memorization rather than generalization\n\n\ncnn\nA convolutional neural network, a type of NN suited for computer vision\n\n\n\n\n\n\n\nArchitecture vs. Model?\n\nA model includes an architecture with a specific set of parameters. These parameters allow the architecture to do something it wasn’t originally designed to do.\n\n\n\n\n\n\nComputer Vision - detection & classification\nText - Classifiction & conversation (but not really)\nTabular - Effective on high cardinality datasets, e.g. part numbers and serial numbers\nRecommendation Systems (Recsys, aka Collaborative Filtering), but note predictions <> recommendations - because you like to read science fiction, a model might predict you’ll like Aasimov - but that might not be what you want from a recommendation engine e.g. if you’re branching out to Romance.\nMulti-modal - Combining the above, capition images, humans in the loop\nVarious other - NLP, protein\n\n\n\n\n\nArsenal 2, combining a camera with an AI platform\nBirdsy, using computer vision to classify birds in real time\n\n\n\n\nThis blog :)\n\n\n\nQuestions 1-12 are annswered in my lesson 1 notes.\n\nWhat is a p value?\nA p-value is the probability of an observed result assuming that the null hypothesis is true. They are terrible and shouldn’t be relied on, per the American Statistical Association. It does not provide a good measure of evidence for scientific conclusions to be made.\nSee p-value\nWhat is a prior?\nNot clear what this question is asking and it’s not listed in the book.\nProvide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\nThe model is not well trained on pictures of bears from various angels. The model might struggle with images of bears from above, or from behind, or partial images.\nWhere do text models currently have a major deficiency?\n\nThis is question 1 of Ch 2. in the book.\n\nWhat are possible negative societal implications of text generation models? In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?\nWhat kind of tabular data is deep learning particularly good at?\nWhat’s a key downside of directly using a deep learning model for recommendation systems?\nIt doesn’t know how to recommend things outside of its domain that you might still be interested in.\nWhat are the steps of the Drivetrain Approach?\nHow do the steps of the Drivetrain Approach map to a recommendation system?\nCreate an image recognition model using data you curate, and deploy it on the web.\nWhat is DataLoaders?\nA class that helps prepare a dataset.\nWhat four things do we need to tell fastai to create DataLoaders?\n\nHow to find the data (get_items)\nHow to get the dependent and independent variable(s) (get_x, get_y)\nCreate the data blocks (e.g. images and labels)\nHow to transform the items, such as Resize(128).\n\nWhat does the splitter parameter to DataBlock do?\nIt defines how your data is split up into a training set and a validation set.\nHow do we ensure a random split always gives the same validation set?\nUse a seed value by speifying seed=int when calling RandomSplitter()."
  },
  {
    "objectID": "posts/2020/09/2020-09-16-lesson-3.html",
    "href": "posts/2020/09/2020-09-16-lesson-3.html",
    "title": "fastai v4: Lesson 3",
    "section": "",
    "text": "Notes from fastai lesson 3, which is Chapter 4 in the book (the end of Ch. 2 is a “choose your own adventure” where you can continue on to Ch. 3, Data Ethics, or Ch. 4, Under the Hood: Training a Digit Classifier). The video lessons pick up at Ch. 4. But After reading much of Ch. 3, I find the Ethics to be an intriguing topic, especially for leaders of data science organizations, where failure to consider Data Ethics might put your organization (or at least its reputation) at risk.\n\n\n\nLesson 3 Video\nfastai Forum\nA visual intro to numpy and data representation\n\n\n\n\n\n\nCovered images as arrays or tensors of pixels\nCool trick:\nVisualizing greyscale images inside of a pandas df by specifying a background graident:\nim3_t = tensor(im3)\ndf = pd.DataFrame(im3_t[4:15,4:22])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n…how a computer might be able to recognize these two digits. What kinds of features might it be able to look at? How might it be able to identify these features? How could it combine them?\n\nThis isn’t a new topic for me as it overlaps with my Signals & Systems coursework, as well as DSP.\n\nMost straightforward: sum(A-B) = similarity score, identical images == 0\nThe above, but after running through edge detection (first derivative)\nUse a kerneling function step-wise compare the outputs of the kernel function between the two systems\n\nMeasuring distance in vectors * L1 norm / mean absolute difference: abs(A-B).mean()\nUse `F.l1_loss()`\n\nL2 norm / root mean square error (RMSE): sqrt(mean((A-B)^2))\nUse F.mse_loss()\n\nWhen computing mean() with tensors, mean() can take an optional tuple argument that specifies the range of axes over which to compute the mean.\nMuch of this chapter covers an introduction to numpy and tensors. Vectors and matrices aren’t new to me, and even slicing n-dimensional arrays is familiar (I have previous experience in perl and C). But I was struggling with this in python, e.g. the difference between [:,:] and [:][:]. I found this article by Jay Alammar very helpful: A visual intro to numpy and data representation (thank you Jay!).\nStochastic Gradient Descent (SGD)\n\n\n\n\n\nSGD\n\n\nSo far our primitive model is little more than a math function with no memory and no feedback. To do better we need to be able to train it and to make changes to a set of weights so as to alter the next outcome of the model.\nWeights for each pixel. Could use the computed means as weights, where the flattened mean are weights.\nConsider the simple function def pr_eight(x,w) = (x*w).sum() where x, w are vectors for the independent variables and the weights, respectively. How would we make this a machine learning “8” classifier?\n\nNeed to init(w) - seeded rand()?\nFor each image in our set, compute x*w e.g. make a prediction.\nCompute the loss of the model (actual vs. expected)\nCompute the gradient: a measure of change for each weight that changes the loss\nUpdate the weights\ngoto 2. until some condition (e.g. the model is good)\n\nInitialize\nStart with random. How would random compare against the mean() though?\nLoss\nThe convention is that small loss == good, big loss == bad. So we want a function that will measure ourselves as such.\nStep\nThis is a change to the weight that results in a new prediction. Weights can go up or down. Mentions using gradients (calculus). But not clear how to avoid things like local minimums…\nStop\nHow we know when to stop - e.g. such as when our model begins to get worse at its job.\n\n\n\n\nGradients are a (continuous? since you must take at least the first derivative) function (used the quadratic y=x**2) with a local minimum (since there’s a stopping point where y’==0?). It’s also symmetric around an axis.\nA random value on the line is chosen (some x) as the initial value\npytorch provides the backward() function to compute the gradient of a layer (array) of a network. This is the backward pass through the network. The forward pass is where the activation function is calculated.\nThis is all handled through object-oriented “magic” in pytorch.\nstepping is done by altering the weights by your learning rate (lr) * the gradient of your weights. Recall that 1e-5 is a common rate.\nTakeaway: the gradient is a search for a local minimum. Step size through this search is the learning rate. This is done by backpropagating the affect of a function to a tensor’s gradient. In this sense, “all the points” in our array are somewhere on this line and we wish to find a tensor such that the sum of the gradients is closest to zero. Think I said that correctly.\n\nApplication:\nLet’s apply the above to our MNIST imageset…\n\n\n\n\n\n\nWhat letters are often used to signify the independent and dependent variables?\nx for the independent variable and y for the dependent variable, as in math, e.g. y = f(x).\nWhat’s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others?\nCrop trims image to size, pad adds zeros (black) to size, squish alters the aspect ratio to size. The choice depends on the data and the subject. It may be advantageous for the model to recognize half a bear, such as if trim is used.\nWhat is data augmentation? Why is it needed?\nData augmentation provides a broader view of material to train on. It can be a way to diversify the training set the model is based on.\nWhat is the difference between item_tfms and batch_tfms?\nitem_tfms operates on each item in a batch while batch_tfms operates on each batch.\nWhat is a confusion matrix?\nA confusion matrix describes the predictions of the model, showing both where it is right vs. where it is wrong. It is shown as a table where x and y are your labels/categories and each cell is the number of images predicted vs. actually labeled.\nWhat does export save?\nExport saves the labels, weights, and biases of the model for use on another system, e.g. to deploy to production.\nWhat is it called when we use a model for getting predictions, instead of training?\nThis is called inference.\nWhat are IPython widgets?\nControls that couple python, javascript, and html into a functional GUI within a jupyter notebook.\nWhen might you want to use CPU for deployment? When might GPU be better?\nFor inference (e.g. use in production), a CPU is likely sufficient, however for models deployed at scale, a GPU may be faster.\nWhat are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?\n\nIt requires online access\nIt is limited by the latency and bandwidth available of the underlying network\nIt requires centralized resources (e.g. the server it runs on)\n\nWhat are three examples of problems that could occur when rolling out a bear warning system in practice?\n\nIt could be fooled by costumes or may not recognize partial bear images\nIt might not know how to recognize all bears\nIf online, requires internet connectivity, if offline, it requires updates\n\nWhat is “out-of-domain data”?\nData shown to a model that is unlike anything it was trained on\nWhat is “domain shift”?\nWhere the data our model is shown in production changes over time, diverging from what it was trained on. To resolve this, online training or simply retraining are used.\nWhat are the three steps in the deployment process? This is question 27 in Chapter 2 of fastbook\n\nCreate (train) the model\nExport the model\nDeploy the model for inference\n\nHow is a grayscale image represented on a computer? How about a color image?\nGrayscale images are represented as an array of 8-bit values. Color images (at least RGB colorspaced) are represented as a tuple/vector of three 8-bit values. For example, the shape of a greyscale image that is 28 pixels by 28 pixels would be (28, 28) and a color image (28, 28, 3).\nHow are the files and folders in the MNIST_SAMPLE dataset structured? Why?\nThe data set has been pre-separated into a training set and a validation set. In each dataset, directories for each numeral contain images of that numeral.\nExplain how the “pixel similarity” approach to classifying digits works.\nThe “pixel similarity” approach used an average of all images of a numeral to create a baseline to compare new numeral images against. This can be done, say, by subtracting the sample image from the average and then using the result to compute an error vector, such as by taking the L1 normal (abs() of the difference minus the mean()).\nWhat is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\nList comprehension is a feature of the python language that returns a list from a some function. For example\ndoubles = [ (x*2) for x in range(1, 100, 2) ]\nReturns a list containing the double of each odd number from 1 up to 100.\nWhat is a “rank-3 tensor”?\nA rank-3 tensor is a matrix. Recall that a rank-0 tensor is a scalar and a rank-1 tensor is a vector.\nWhat is the difference between tensor rank and shape? How do you get the rank from the shape?\nA tensor shape describes the length of a tensor in each of its dimensions. The rank describes the number of dimensions. The rank can be computed as:\nrank = len(t.shape)\nWhat are RMSE and L1 norm?\nL1 norm is the mean absolute distance: abs(A-B) - mean(A-B) L2 norm is the root mean square average: (A-B)**2.mean().sqrt()\nHow can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\nUsing matrix math in numpy or tensors in pytorch.\nCreate a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n(t.tensor(range(1,10)).reshape(3, 3)*2)[1:3,1:3]\n\ntensor([[10,  12],\n         16, 18]])\nWhat is broadcasting?\nAre metrics generally calculated using the training set, or the validation set? Why?\nWhat is SGD?\nSGD is stochastic gradient descent.\nWhy does SGD use mini-batches?\nWe can more quickly compute new parameters with smaller steps. These smaller steps are mini-batches. They are less accurate than running the full batch, but faster. They are also more likely to be runnable on GPU accelerated systems, where memory is limited.\nWhat are the seven steps in SGD for machine learning?\n\ninit: Initialize our weights\npredict: For a given weight, predict the next value\nloss: Compute the loss from our prediction\ngradient: Compute the gradient for how much our loss would change for a small change to a weight\nstep: Update our weight\nrepeat: return to predict to continue the process\nstop: cease learning\n\nHow do we initialize the weights in a model?\nWhat is “loss”?\nLoss is a computation of how good the model is doing by measuring predictions vs. actuals.\nWhy can’t we always use a high learning rate?\nIf the model learns too quickly it will pass or oscillate around the optimal weights. Also, if the learning rate is too high, we may overfit our model.\nWhat is a “gradient”?\nA gradient is a computation of how much our loss would change by making a small change to a weight."
  },
  {
    "objectID": "posts/2022/09/2022-09-24-homelab-troubleshooting.html",
    "href": "posts/2022/09/2022-09-24-homelab-troubleshooting.html",
    "title": "Homelab Troubleshooting",
    "section": "",
    "text": "I spent today debugging several issues I’ve had with a server in my homelab. The server happens to be part of a proxmox cluster, making the issues more annoying.\n\n\nI have four SATA HDDs in a USB-C enclosure attached to the host. The four drives are part of a zfs raidz volume that are identified by their World Wide Name (wwn).\nOccassionally, when the system reboots or powers on, only three of the four disks have a their wwn- symlink appear in /dev/disk/by-id/.\nlrwxrwxrwx 1 root root  9 Sep 24 09:41 wwn-0x50014ee210f452db -> ../../sdk\nlrwxrwxrwx 1 root root 10 Sep 24 09:41 wwn-0x50014ee210f452db-part1 -> ../../sdk1\nlrwxrwxrwx 1 root root 10 Sep 24 09:41 wwn-0x50014ee210f452db-part9 -> ../../sdk9\nlrwxrwxrwx 1 root root  9 Sep 24 09:41 wwn-0x50014ee2664999d1 -> ../../sdj\nlrwxrwxrwx 1 root root 10 Sep 24 09:41 wwn-0x50014ee2664999d1-part1 -> ../../sdj1\nlrwxrwxrwx 1 root root 10 Sep 24 09:41 wwn-0x50014ee2664999d1-part9 -> ../../sdj9\nlrwxrwxrwx 1 root root  9 Sep 24 09:41 wwn-0x50014ee2bb9f24e0 -> ../../sdl\nlrwxrwxrwx 1 root root 10 Sep 24 09:41 wwn-0x50014ee2bb9f24e0-part1 -> ../../sdl1\nlrwxrwxrwx 1 root root 10 Sep 24 09:41 wwn-0x50014ee2bb9f24e0-part9 -> ../../sdl9\nlrwxrwxrwx 1 root root  9 Sep 24 09:41 wwn-0x50025385a013ece8 -> ../../sdi\nThis results in the pool being degraded:\nroot@pve1:/sys/bus/pci_express/devices# zpool status\n  pool: tank\n state: DEGRADED\nstatus: One or more devices could not be used because the label is missing or\n        invalid.  Sufficient replicas exist for the pool to continue\n        functioning in a degraded state.\naction: Replace the device using 'zpool replace'.\n   see: https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-4J\n  scan: resilvered 1.19G in 00:00:28 with 0 errors on Tue Sep 20 08:55:45 2022\nconfig:\n\n        NAME                        STATE     READ WRITE CKSUM\n        tank                        DEGRADED     0     0     0\n          raidz1-0                  DEGRADED     0     0     0\n            wwn-0x50014ee2664999d1  ONLINE       0     0     0\n            wwn-0x50014ee210f452db  ONLINE       0     0     0\n            wwn-0x50014ee2bb9f24e0  ONLINE       0     0     0\n            wwn-0x50014ee210f434b7  UNAVAIL      0     0     0\n\nerrors: No known data errors\nUp until today I’ve been lazy and solving it by shutting down the system, powering down the enclosure, powering up the enclosure, and then turning on the system. This has reliably ensured that the enclosure and pool starts correctly.\nroot@pve1:/sys/class/scsi_disk# echo \"- - -\" | tee /sys/class/scsi_host/host1{0,1,2,3}/scan\n- - -\nroot@pve1:/sys/class/scsi_disk# sudo udevadm trigger\nAfterwards in /dev/disk/by-id/.\nlrwxrwxrwx 1 root root  9 Sep 24 09:56 wwn-0x50014ee210f434b7 -> ../../sdm\nlrwxrwxrwx 1 root root 10 Sep 24 09:56 wwn-0x50014ee210f434b7-part1 -> ../../sdm1\nlrwxrwxrwx 1 root root 10 Sep 24 09:56 wwn-0x50014ee210f434b7-part9 -> ../../sdm9\nlrwxrwxrwx 1 root root  9 Sep 24 09:56 wwn-0x50014ee210f452db -> ../../sdk\nlrwxrwxrwx 1 root root 10 Sep 24 09:56 wwn-0x50014ee210f452db-part1 -> ../../sdk1\nlrwxrwxrwx 1 root root 10 Sep 24 09:56 wwn-0x50014ee210f452db-part9 -> ../../sdk9\nlrwxrwxrwx 1 root root  9 Sep 24 09:56 wwn-0x50014ee2664999d1 -> ../../sdj\nlrwxrwxrwx 1 root root 10 Sep 24 09:56 wwn-0x50014ee2664999d1-part1 -> ../../sdj1\nlrwxrwxrwx 1 root root 10 Sep 24 09:56 wwn-0x50014ee2664999d1-part9 -> ../../sdj9\nlrwxrwxrwx 1 root root  9 Sep 24 09:56 wwn-0x50014ee2bb9f24e0 -> ../../sdl\nlrwxrwxrwx 1 root root 10 Sep 24 09:56 wwn-0x50014ee2bb9f24e0-part1 -> ../../sdl1\nlrwxrwxrwx 1 root root 10 Sep 24 09:56 wwn-0x50014ee2bb9f24e0-part9 -> ../../sdl9\nAnd the array automatically resilvers:\nroot@pve1:~# zpool status\n  pool: tank\n state: ONLINE\n  scan: resilvered 29.6M in 00:00:03 with 0 errors on Sat Sep 24 09:56:38 2022\nconfig:\n\n        NAME                        STATE     READ WRITE CKSUM\n        tank                        ONLINE       0     0     0\n          raidz1-0                  ONLINE       0     0     0\n            wwn-0x50014ee2664999d1  ONLINE       0     0     0\n            wwn-0x50014ee210f452db  ONLINE       0     0     0\n            wwn-0x50014ee2bb9f24e0  ONLINE       0     0     0\n            wwn-0x50014ee210f434b7  ONLINE       0     0     0\n\nerrors: No known data errors\n\n\n\nMy server has a serial port that I can use as a console, but I was surprised to find that after systemd began starting up, the console would stop working. grub and the kernel output worked fine.\nI checked the serial port and noticed that the baud rate switched from the kernel command line setting of `` to a much lower speed:\nroot@pve1:~# stty -a -F /dev/ttyS0\nspeed 1200 baud; rows 24; columns 80; line = 0;\nintr = ^C; quit = ^\\; erase = ^?; kill = ^U; eof = ^D; eol = <undef>; eol2 = <undef>; swtch = <undef>; start = ^Q;\nstop = ^S; susp = ^Z; rprnt = ^R; werase = ^W; lnext = ^V; discard = ^O; min = 0; time = 5;\n-parenb -parodd -cmspar cs8 -hupcl -cstopb cread clocal -crtscts\nignbrk -brkint ignpar -parmrk -inpck -istrip -inlcr -igncr -icrnl -ixon -ixoff -iuclc -ixany -imaxbel -iutf8\n-opost -olcuc -ocrnl -onlcr -onocr -onlret -ofill -ofdel nl0 cr0 tab0 bs0 vt0 ff0\n-isig -icanon -iexten -echo -echoe -echok -echonl -noflsh -xcase -tostop -echoprt -echoctl -echoke -flusho -extproc\nroot@pve1:~# stty -a -F /dev/ttyS0\nspeed 2400 baud; rows 24; columns 80; line = 0;\nintr = ^C; quit = ^\\; erase = ^?; kill = ^U; eof = ^D; eol = <undef>; eol2 = <undef>; swtch = <undef>; start = ^Q;\nstop = ^S; susp = ^Z; rprnt = ^R; werase = ^W; lnext = ^V; discard = ^O; min = 0; time = 5;\n-parenb -parodd -cmspar cs8 -hupcl -cstopb cread clocal -crtscts\nignbrk -brkint ignpar -parmrk -inpck -istrip -inlcr -igncr -icrnl -ixon -ixoff -iuclc -ixany -imaxbel -iutf8\n-opost -olcuc -ocrnl -onlcr -onocr -onlret -ofill -ofdel nl0 cr0 tab0 bs0 vt0 ff0\n-isig -icanon -iexten -echo -echoe -echok -echonl -noflsh -xcase -tostop -echoprt -echoctl -echoke -flusho -extproc\nSetting the baud rate manually to 115200 would make it briefly work:\nroot@pve1:~# stty -F /dev/ttyS0 115200\nBut after 10 seconds or so the console would revert back to 1200 and the output in the console would not work.\nTurns out the problem was my UPS monitoring software, pwrstatd:\nroot@pve1:~# lsof -n | grep ttyS0\npwrstatd   4892                             root    4u      CHR               4,64          0t0         89 /dev/ttyS0\nagetty     4927                             root    0u      CHR               4,64          0t0         89 /dev/ttyS0\nagetty     4927                             root    1u      CHR               4,64          0t0         89 /dev/ttyS0\nagetty     4927                             root    2u      CHR               4,64          0t0         89 /dev/ttyS0\nagetty     4927                             root    4r  a_inode               0,14            0      12461 inotify\nI modified /etc/pwrstatd.conf:\nallowed-device-nodes = libusb\nBut that didn’t fix it. I tried another setting:\nallowed-device-nodes = libusb;hiddev;ttyUSB\nAnd the service stopped trying to access /dev/ttyS0:\nroot@pve1:~# lsof -n | grep ttyS0\nagetty    89648                             root    0u      CHR               4,64          0t0         89 /dev/ttyS0\nagetty    89648                             root    1u      CHR               4,64          0t0         89 /dev/ttyS0\nagetty    89648                             root    2u      CHR               4,64          0t0         89 /dev/ttyS0\nYay. Restarted getty and success!\nroot@pve1:~# systemctl stop serial-getty@ttyS0.service\nroot@pve1:~# systemctl start serial-getty@ttyS0.service\n\n\n\nThe same proxmox server runs a headless setup; I somehow got it to boot even though a Mellanox ConnectX-3 adapter is installed.\nAfter updating the kernel I noticed that the system didn’t come back up. The serial console didn’t even show output from grub, so there was either a significant hardware problem or the bootloader had somehow become corrupted. I tried power cycling it several times but it would never boot. Finally I replaced the Mellanox card with an old video card so I could see what’s going on.\nApparently during a previous power cycle the LSI Megaraid 9260-8i card had an unclean shutdown and was not able to flush its write cache. The card decided to let me know by prompting me to press any key to continue or to press ‘C’ to enter the configuration utiltiy. The prompt is a one time event (per occurrence, I’m sure), so after pressing spacebar and verifying grub came up, I tested that grub would come up after a reboot, then restored the Mellanox card. The system booted up fine after that.\nCache data was lost due to an unexpected power-off or reboot during a write operation, but the adapter has recovered. This could be due to a memory problem, bad battery, or you may not have a bettery installed.\nPress any key to continue or 'C' to load the configuration utility.\nRecently the LSI card had a BBU-iBBU08 battery pack installed, but I removed it when I noticed significant bulging on the side of the battery pack. I bought the battery pack three years ago so it had a good run."
  },
  {
    "objectID": "posts/migration/index.html",
    "href": "posts/migration/index.html",
    "title": "Migration from fastpages",
    "section": "",
    "text": "Quarto works a bit different than fastpages, but I’m excited that it’s being actively developed.\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I recieved a degree in Electrical Engineering from UCSD in 2007. I’ve worked most of my engineering career at Viasat, where I’ve enjoyed career mobility by being a test engineer, software engineer, and hardware engineer, later becoming a cross-functional leader of an engineering organization comprised of mechanical, hardware (including programmable logic), and software engineers. Much of my careeer has been focused on constructing what would now be considered “well-labeled datasets”, as well as web-based analysis tools for computing process capability metrics, supporting root cause analysis, providing insight into product performance, general process automation, etc.\nSince learning more about data science and machine learning, I’ve begun exploring the intersection of ML and manufacturing, particularly how ML can accelerate manufacturing & test processes. I feel passionately that this technology can be used to improve product design by providing greater context and understanding by combining manufacturing, test, and operational datasets.\nI first started diving into machine learning myself using the fast.ai v3 course, but switched to v4 of the course when it was recently released. In Lesson 3 of v4 of the course, Jeremy recommends to write about what you are learning. Once I saw fastpages[^1], I figured it was time to start writing. They had me once I found out I could convert jupyter notebooks to blog posts!\nI created this site to help me document my journey, study, challenge myself, and show off what I’ve learned."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tbeck.io/blog",
    "section": "",
    "text": "homelab\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\nTim Beck\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\nTim Beck\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\neducation\n\n\nfastai\n\n\n\n\nNotes from Lesson 3 of fastai v4\n\n\n\n\n\n\nSep 16, 2020\n\n\nTim Beck\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neducation\n\n\nfastai\n\n\n\n\nNotes from Lesson 2 of fastai v4\n\n\n\n\n\n\nSep 6, 2020\n\n\nTim Beck\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neducation\n\n\nfastai\n\n\n\n\nReview & notes from the first two lessons of fastai v4.\n\n\n\n\n\n\nSep 5, 2020\n\n\nTim Beck\n\n\n\n\n\n\nNo matching items\n\n\nAll posts"
  },
  {
    "objectID": "index.html#notebooks",
    "href": "index.html#notebooks",
    "title": "tbeck.io/blog",
    "section": "Notebooks",
    "text": "Notebooks"
  }
]